{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd163806-0066-4bff-8d99-20704de2f881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T09:56:07.806088Z",
     "iopub.status.busy": "2022-04-22T09:56:07.805584Z",
     "iopub.status.idle": "2022-04-22T09:56:07.813488Z",
     "shell.execute_reply": "2022-04-22T09:56:07.811934Z",
     "shell.execute_reply.started": "2022-04-22T09:56:07.806039Z"
    },
    "tags": []
   },
   "source": [
    "## Assignment 4 - Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d3b08-3033-4dee-866d-1ab2e74c91b8",
   "metadata": {},
   "source": [
    "The assignment for this week builds on these concepts and techniques. We're going to be working with the data in the folder CDS-LANG/toxic and trying to see if we can predict whether or not a comment is a certain kind of toxic speech. You should write two scripts which do the following:\n",
    "\n",
    "-->The first script should perform benchmark classification using standard machine learning approaches\n",
    "This means CountVectorizer() or TfidfVectorizer(), LogisticRegression classifier\n",
    "\n",
    "-Save the results from the classification report to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e3b874-50b3-4cc4-9484-1fbac8a47a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-24T14:21:43.566236Z",
     "iopub.status.busy": "2022-04-24T14:21:43.565706Z",
     "iopub.status.idle": "2022-04-24T14:22:19.877250Z",
     "shell.execute_reply": "2022-04-24T14:22:19.875698Z",
     "shell.execute_reply.started": "2022-04-24T14:21:43.566185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (4.9.3)\n",
      "Collecting contractions\n",
      "  Using cached contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.17.3)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.21.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.10.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.13.0)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m299.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (49.6.0.post20210108)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m421.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.7.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.34.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.4)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.1)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=e7a59d0e134885bd9a889c94994bcec5a8180191e348bd8aef80b9f431307f6d\n",
      "  Stored in directory: /home/ucloud/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, pyahocorasick, libclang, keras, flatbuffers, wrapt, threadpoolctl, tensorflow-io-gcs-filesystem, tensorboard-data-server, opt-einsum, keras-preprocessing, joblib, google-pasta, gast, astunparse, anyascii, textsearch, scikit-learn, nltk, contractions, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "Successfully installed anyascii-0.3.1 astunparse-1.6.3 contractions-0.1.68 flatbuffers-2.0 gast-0.5.3 google-pasta-0.2.0 joblib-1.1.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 nltk-3.7 opt-einsum-3.3.0 pyahocorasick-1.4.4 scikit-learn-1.0.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 textsearch-0.0.21 tf-estimator-nightly-2.8.0.dev2021122109 threadpoolctl-3.1.0 wrapt-1.14.0\n"
     ]
    }
   ],
   "source": [
    "#setup script wasn't working in class, so:\n",
    "!pip install nltk beautifulsoup4 contractions tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160f01fd-8f29-4b55-a1ea-acdfb8efdc91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:29.417839Z",
     "iopub.status.busy": "2022-04-28T20:25:29.416743Z",
     "iopub.status.idle": "2022-04-28T20:25:30.084708Z",
     "shell.execute_reply": "2022-04-28T20:25:30.083532Z",
     "shell.execute_reply.started": "2022-04-28T20:25:29.417782Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to train a Logistic Regression model using scikit-learn, from vis_assign_2\n",
    "\n",
    "#session_8\n",
    "# system tools\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\", \"..\", \"CDS-LANG\"))\n",
    "\n",
    "# data munging tools\n",
    "import pandas as pd\n",
    "import utils.classifier_utils as clf\n",
    "\n",
    "# Machine learning stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#surpress warnings\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" #ignore warnings\n",
    "    \n",
    "#scikit-learn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# data wranling\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed760b6-f979-42aa-9bdc-a5a35de361a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:30.600519Z",
     "iopub.status.busy": "2022-04-28T20:25:30.599895Z",
     "iopub.status.idle": "2022-04-28T20:25:30.606626Z",
     "shell.execute_reply": "2022-04-28T20:25:30.605648Z",
     "shell.execute_reply.started": "2022-04-28T20:25:30.600447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data:\n",
    "# get the filepath for test labels\n",
    "filepath = os.path.join(\"..\",\"..\",\"CDS-LANG\",\"toxic\",\"VideoCommentsThreatCorpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ff83f5-45a2-4b2c-905a-83d469255c0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:33.783610Z",
     "iopub.status.busy": "2022-04-28T20:25:33.782934Z",
     "iopub.status.idle": "2022-04-28T20:25:33.832170Z",
     "shell.execute_reply": "2022-04-28T20:25:33.831140Z",
     "shell.execute_reply.started": "2022-04-28T20:25:33.783556Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#open csv with pandas\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591919a3-9697-4fad-a944-a483ac5031f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:34.914852Z",
     "iopub.status.busy": "2022-04-28T20:25:34.913844Z",
     "iopub.status.idle": "2022-04-28T20:25:34.927898Z",
     "shell.execute_reply": "2022-04-28T20:25:34.926814Z",
     "shell.execute_reply.started": "2022-04-28T20:25:34.914797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                               text\n",
      "0          0  It's because Europeans do not want to change t...\n",
      "1          0  The Muslims there do not want to assimilate pr...\n",
      "2          1  But it's ok....because Europe will soon rebel ...\n",
      "3          0  I forsee a big civil war in Europe in the futu...\n",
      "4          0  ISLAM – A Simple, Humanitarian and Attractive ...\n",
      "...      ...                                                ...\n",
      "28638      1  yeah we are all monsters..I'm gonna kill u rig...\n",
      "28639      0                       stupid brainwashed idiot..\\n\n",
      "28640      0  have you EVER been to Serbia or kosovo...fucki...\n",
      "28641      0  probably u mean to this monsters, fucker /watc...\n",
      "28642      0  the fucking funniest thing is that fucking ame...\n",
      "\n",
      "[28643 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#looking at the dataset\n",
    "print(data)\n",
    "#1 = toxic, 0= non-toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fe9d0f-9244-4292-a81d-59e60ef6072a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:38.409179Z",
     "iopub.status.busy": "2022-04-28T20:25:38.408541Z",
     "iopub.status.idle": "2022-04-28T20:25:38.425972Z",
     "shell.execute_reply": "2022-04-28T20:25:38.425287Z",
     "shell.execute_reply.started": "2022-04-28T20:25:38.409122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create more balanced dataset, getting a random sample of the dataset\n",
    "data_balanced = clf.balance(data, 1000) #1000 datapoints for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0016cd7-4aaf-4661-9b3a-8e3871b8a211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:39.417519Z",
     "iopub.status.busy": "2022-04-28T20:25:39.416871Z",
     "iopub.status.idle": "2022-04-28T20:25:39.424117Z",
     "shell.execute_reply": "2022-04-28T20:25:39.422899Z",
     "shell.execute_reply.started": "2022-04-28T20:25:39.417453Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create new variables called text and label\n",
    "# taking the data out of the dataframe so that we can mess around with them.\n",
    "X = data_balanced[\"text\"] #text column\n",
    "y = data_balanced[\"label\"] #label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c9359e-6ba4-4f12-8409-c77b7d2ebf00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:40.393689Z",
     "iopub.status.busy": "2022-04-28T20:25:40.392966Z",
     "iopub.status.idle": "2022-04-28T20:25:40.404522Z",
     "shell.execute_reply": "2022-04-28T20:25:40.403262Z",
     "shell.execute_reply.started": "2022-04-28T20:25:40.393611Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, #texts for the model\n",
    "                                                    y, #classification labels\n",
    "                                                    test_size = 0.2, #create an 80/20 split (testing to be 20% of the overall data)\n",
    "                                                    random_state = 42) #where we should start: just for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1838115a-ed33-4c17-ae77-48e34859a05e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:41.453319Z",
     "iopub.status.busy": "2022-04-28T20:25:41.452854Z",
     "iopub.status.idle": "2022-04-28T20:25:41.460744Z",
     "shell.execute_reply": "2022-04-28T20:25:41.459811Z",
     "shell.execute_reply.started": "2022-04-28T20:25:41.453272Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create vectorizer object\n",
    "#can modify all of these parameters (or not set any)\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,2), #the size of tokens: unigrams and bigrams (1: individual words and 2 word pairings: repeated word pattern)\n",
    "                             lowercase = True, #use lowercase: getting all instances of the word\n",
    "                             max_df = 0.95, #remove very common words, stop words, if these words appear in over 95% of the documents= gone\n",
    "                             min_df = 0.05, #remove very rare words, occur in fewer than 0.05% of the documents= gone, does not \"discriminate\"\n",
    "                             max_features = 100) #keep only top 500 features (compress to focus on the 500 most frequent words- further filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142544f3-9b57-421e-b776-90086b896392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:44.437192Z",
     "iopub.status.busy": "2022-04-28T20:25:44.436674Z",
     "iopub.status.idle": "2022-04-28T20:25:44.564751Z",
     "shell.execute_reply": "2022-04-28T20:25:44.564138Z",
     "shell.execute_reply.started": "2022-04-28T20:25:44.437144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This vectorizer is then used to turn all of our documents into a vector of numbers, instead of text.\n",
    "#use on all documents\n",
    "#first we fit to the training data\n",
    "X_train_feats = vectorizer.fit_transform(X_train)\n",
    "\n",
    "#we then do it for our test data\n",
    "X_test_feats = vectorizer.transform(X_test) #no fitting!! as we are trying to predict\n",
    "\n",
    "#get feature names: the unigrams and bigrams\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "311907d6-b829-49b0-aa80-02a926111eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:45.608615Z",
     "iopub.status.busy": "2022-04-28T20:25:45.608074Z",
     "iopub.status.idle": "2022-04-28T20:25:45.630016Z",
     "shell.execute_reply": "2022-04-28T20:25:45.629165Z",
     "shell.execute_reply.started": "2022-04-28T20:25:45.608565Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Classifying and predicting\n",
    "classifier = LogisticRegression(random_state=42).fit(X_train_feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4741b94-b1dc-4471-b3c9-520571c1163c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:46.679064Z",
     "iopub.status.busy": "2022-04-28T20:25:46.678595Z",
     "iopub.status.idle": "2022-04-28T20:25:46.685750Z",
     "shell.execute_reply": "2022-04-28T20:25:46.684665Z",
     "shell.execute_reply.started": "2022-04-28T20:25:46.679017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use the classifier to make predictions\n",
    "y_pred = classifier.predict(X_test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357a14a3-7138-4f7c-b327-dec3030cba81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:25:51.035615Z",
     "iopub.status.busy": "2022-04-28T20:25:51.035084Z",
     "iopub.status.idle": "2022-04-28T20:25:51.050262Z",
     "shell.execute_reply": "2022-04-28T20:25:51.049402Z",
     "shell.execute_reply.started": "2022-04-28T20:25:51.035566Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74       199\n",
      "           1       0.77      0.64      0.70       201\n",
      "\n",
      "    accuracy                           0.72       400\n",
      "   macro avg       0.73      0.72      0.72       400\n",
      "weighted avg       0.73      0.72      0.72       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "classifier_metrics = metrics.classification_report(y_test, y_pred)\n",
    "print(classifier_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3a96cda-379a-4f9c-b78d-22dd7f035eeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:26:07.663347Z",
     "iopub.status.busy": "2022-04-28T20:26:07.662887Z",
     "iopub.status.idle": "2022-04-28T20:26:07.671515Z",
     "shell.execute_reply": "2022-04-28T20:26:07.670495Z",
     "shell.execute_reply.started": "2022-04-28T20:26:07.663299Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! classifier_metrics has been generated and saved in the output folder as benchmark_assign_4.txt\n"
     ]
    }
   ],
   "source": [
    "#Save the results from the classification report to a text file\n",
    "g = open(\"../../cds-lang/Lang-assignments/output/benchmark_assign_4.txt\",'w')\n",
    "print(classifier_metrics, file=g)\n",
    "\n",
    "print(\"Done! classifier_metrics has been generated and saved in the output folder as benchmark_assign_4.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
