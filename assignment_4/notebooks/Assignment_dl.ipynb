{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298c9d99-bdf7-4c85-9db6-595a68d726c1",
   "metadata": {},
   "source": [
    "## Assignment 4 - Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7251ec-b603-4bdf-bc95-2f598d4c3f17",
   "metadata": {},
   "source": [
    "The assignment for this week builds on these concepts and techniques. We're going to be working with the data in the folder CDS-LANG/toxic and trying to see if we can predict whether or not a comment is a certain kind of toxic speech. You should write two scripts which do the following:\n",
    "\n",
    "\n",
    "-->The second script should perform classification using the kind of deep learning methods we saw in class\n",
    "Keras Embedding layer, Convolutional Neural Network\n",
    "\n",
    "-Save the classification report to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86536379-de57-40b9-b94c-4972187986f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T09:45:42.283824Z",
     "iopub.status.busy": "2022-04-22T09:45:42.283302Z",
     "iopub.status.idle": "2022-04-22T09:46:16.815295Z",
     "shell.execute_reply": "2022-04-22T09:46:16.813810Z",
     "shell.execute_reply.started": "2022-04-22T09:45:42.283774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (4.9.3)\n",
      "Collecting contractions\n",
      "  Using cached contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m315.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.39.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.10.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (0.13.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.21.2)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 KB\u001b[0m \u001b[31m822.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m381.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m524.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.7.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.34.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.4)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 KB\u001b[0m \u001b[31m628.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.1)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=0a9ac5ae835643ff0fbec59971ba06de71ac78ba695648230e27147e9b271481\n",
      "  Stored in directory: /home/ucloud/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, pyahocorasick, libclang, keras, flatbuffers, wrapt, threadpoolctl, tensorflow-io-gcs-filesystem, tensorboard-data-server, opt-einsum, keras-preprocessing, joblib, google-pasta, gast, astunparse, anyascii, textsearch, scikit-learn, nltk, contractions, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "Successfully installed anyascii-0.3.1 astunparse-1.6.3 contractions-0.1.68 flatbuffers-2.0 gast-0.5.3 google-pasta-0.2.0 joblib-1.1.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 nltk-3.7 opt-einsum-3.3.0 pyahocorasick-1.4.4 scikit-learn-1.0.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 textsearch-0.0.21 tf-estimator-nightly-2.8.0.dev2021122109 threadpoolctl-3.1.0 wrapt-1.14.0\n"
     ]
    }
   ],
   "source": [
    "#setup script wasn't working in class, so:\n",
    "!pip install nltk beautifulsoup4 contractions tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a988bc-85de-422a-81a7-3d8ce057197d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:15.327605Z",
     "iopub.status.busy": "2022-04-28T20:31:15.327053Z",
     "iopub.status.idle": "2022-04-28T20:31:18.167817Z",
     "shell.execute_reply": "2022-04-28T20:31:18.167166Z",
     "shell.execute_reply.started": "2022-04-28T20:31:15.327552Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "2022-04-28 22:31:16.761933: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-28 22:31:16.761971: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# simple text processing tools\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup #remove things that are non-text\n",
    "import nltk #we used spacy in the past\n",
    "nltk.download('punkt')\n",
    "\n",
    "# data wranling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, \n",
    "                                    Flatten,\n",
    "                                    Conv1D, \n",
    "                                    MaxPooling1D, \n",
    "                                    Embedding)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                            classification_report)\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "# Machine learning stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "# visualisations \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d92b57-584d-4f5c-9dd6-7d750da02567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:18.502664Z",
     "iopub.status.busy": "2022-04-28T20:31:18.502188Z",
     "iopub.status.idle": "2022-04-28T20:31:18.515904Z",
     "shell.execute_reply": "2022-04-28T20:31:18.514858Z",
     "shell.execute_reply.started": "2022-04-28T20:31:18.502614Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Helper functions for text processing\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text #everything is in English: no accented characters in English. When used here, it could be used incorrectly/inconsistently\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower() #lower case\n",
    "    doc = remove_accented_chars(doc) #no accented characters\n",
    "    doc = contractions.fix(doc) #resolves contractions: you're -> you are\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d920f9-a4ab-4eb6-aa18-ff71687aeaa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:22.003324Z",
     "iopub.status.busy": "2022-04-28T20:31:22.002799Z",
     "iopub.status.idle": "2022-04-28T20:31:22.010117Z",
     "shell.execute_reply": "2022-04-28T20:31:22.008925Z",
     "shell.execute_reply.started": "2022-04-28T20:31:22.003274Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data:\n",
    "# get the filepath\n",
    "filepath = os.path.join(\"..\",\"..\",\"CDS-LANG\",\"toxic\",\"VideoCommentsThreatCorpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd72525-f07a-45d3-8d39-1a3b40b2d351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:23.226384Z",
     "iopub.status.busy": "2022-04-28T20:31:23.225895Z",
     "iopub.status.idle": "2022-04-28T20:31:23.275390Z",
     "shell.execute_reply": "2022-04-28T20:31:23.274773Z",
     "shell.execute_reply.started": "2022-04-28T20:31:23.226334Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#open csv with pandas\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f164b6f-bc8f-4604-8a3d-b673e14e2f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:24.477277Z",
     "iopub.status.busy": "2022-04-28T20:31:24.476777Z",
     "iopub.status.idle": "2022-04-28T20:31:24.490572Z",
     "shell.execute_reply": "2022-04-28T20:31:24.489830Z",
     "shell.execute_reply.started": "2022-04-28T20:31:24.477226Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                               text\n",
      "0          0  It's because Europeans do not want to change t...\n",
      "1          0  The Muslims there do not want to assimilate pr...\n",
      "2          1  But it's ok....because Europe will soon rebel ...\n",
      "3          0  I forsee a big civil war in Europe in the futu...\n",
      "4          0  ISLAM – A Simple, Humanitarian and Attractive ...\n",
      "...      ...                                                ...\n",
      "28638      1  yeah we are all monsters..I'm gonna kill u rig...\n",
      "28639      0                       stupid brainwashed idiot..\\n\n",
      "28640      0  have you EVER been to Serbia or kosovo...fucki...\n",
      "28641      0  probably u mean to this monsters, fucker /watc...\n",
      "28642      0  the fucking funniest thing is that fucking ame...\n",
      "\n",
      "[28643 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#looking at the dataset\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6286e6-1277-49ef-9f54-8aab54a28ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:27.441311Z",
     "iopub.status.busy": "2022-04-28T20:31:27.440775Z",
     "iopub.status.idle": "2022-04-28T20:31:27.448327Z",
     "shell.execute_reply": "2022-04-28T20:31:27.447042Z",
     "shell.execute_reply.started": "2022-04-28T20:31:27.441259Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create new variables called text and label\n",
    "# taking the data out of the dataframe so that we can mess around with them.\n",
    "X = data[\"text\"] #text column\n",
    "y = data[\"label\"] #label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0113fd2a-efa3-4450-b0bd-a0bfdda8da22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:28.554705Z",
     "iopub.status.busy": "2022-04-28T20:31:28.554217Z",
     "iopub.status.idle": "2022-04-28T20:31:28.568934Z",
     "shell.execute_reply": "2022-04-28T20:31:28.568177Z",
     "shell.execute_reply.started": "2022-04-28T20:31:28.554657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, #texts for the model\n",
    "                                                    y, #classification labels\n",
    "                                                    test_size = 0.2, #create an 80/20 split (testing to be 20% of the overall data)\n",
    "                                                    random_state = 42) #where we should start: just for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acc0d42-ab10-4c9a-bf4f-6af79aaebaae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:52.965440Z",
     "iopub.status.busy": "2022-04-28T20:31:52.964909Z",
     "iopub.status.idle": "2022-04-28T20:31:55.345871Z",
     "shell.execute_reply": "2022-04-28T20:31:55.345230Z",
     "shell.execute_reply.started": "2022-04-28T20:31:52.965389Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22914/22914 [00:01<00:00, 12165.07it/s]\n",
      "100%|██████████| 5729/5729 [00:00<00:00, 11854.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#clean and normalize data (lots of noise like html tags) see helper cell\n",
    "X_train_norm = pre_process_corpus(X_train)\n",
    "X_test_norm = pre_process_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2f5cf1-1509-45d7-a7b5-a8dd56d28aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:31:55.387064Z",
     "iopub.status.busy": "2022-04-28T20:31:55.386933Z",
     "iopub.status.idle": "2022-04-28T20:31:55.390977Z",
     "shell.execute_reply": "2022-04-28T20:31:55.390496Z",
     "shell.execute_reply.started": "2022-04-28T20:31:55.387049Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more like the holy qurap\n"
     ]
    }
   ],
   "source": [
    "#looking at the first comment\n",
    "print(X_train_norm[0]) #the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907eea49-1285-43a2-b71b-3a3db666c6cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:01.127254Z",
     "iopub.status.busy": "2022-04-28T20:32:01.126723Z",
     "iopub.status.idle": "2022-04-28T20:32:01.438732Z",
     "shell.execute_reply": "2022-04-28T20:32:01.437742Z",
     "shell.execute_reply.started": "2022-04-28T20:32:01.127203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenize sequences\n",
    "# creates index of every word in doc, converts all word to number in index in training data\n",
    "\n",
    "#define out-of-vocabulary-token\n",
    "t = Tokenizer(oov_token = \"<UNK>\")\n",
    "#model has not encountered during training= unknown\n",
    "              \n",
    "#fit the tokenizer on the documents\n",
    "t.fit_on_texts(X_train_norm)\n",
    "\n",
    "#set padding value (different lengths of documents etc. need a max doument length. If shorter= padding of zeros)\n",
    "t.word_index[\"<PAD>\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f09594e-a40f-4543-92ad-2acba2a80f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:02.619101Z",
     "iopub.status.busy": "2022-04-28T20:32:02.618597Z",
     "iopub.status.idle": "2022-04-28T20:32:02.889982Z",
     "shell.execute_reply": "2022-04-28T20:32:02.889383Z",
     "shell.execute_reply.started": "2022-04-28T20:32:02.619052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenize all documents using this fit tokenizer\n",
    "#sequence: anything that can be iterated over\n",
    "X_train_seqs = t.texts_to_sequences(X_train_norm)\n",
    "X_test_seqs = t.texts_to_sequences(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad1141b-465a-41d5-919c-51b0312b3b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:05.601425Z",
     "iopub.status.busy": "2022-04-28T20:32:05.600895Z",
     "iopub.status.idle": "2022-04-28T20:32:05.607899Z",
     "shell.execute_reply": "2022-04-28T20:32:05.606650Z",
     "shell.execute_reply.started": "2022-04-28T20:32:05.601374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sequence normalization\n",
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aecbf6a6-31f4-44bb-a388-89212b3c402f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:06.698830Z",
     "iopub.status.busy": "2022-04-28T20:32:06.698307Z",
     "iopub.status.idle": "2022-04-28T20:32:06.812440Z",
     "shell.execute_reply": "2022-04-28T20:32:06.811194Z",
     "shell.execute_reply.started": "2022-04-28T20:32:06.698781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add padding to sequences\n",
    "X_train_pad = sequence.pad_sequences(X_train_seqs, maxlen= MAX_SEQUENCE_LENGTH)\n",
    "X_test_pad = sequence.pad_sequences(X_test_seqs, maxlen= MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cb05807-4c89-4272-aa87-451864cdc2d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:07.795015Z",
     "iopub.status.busy": "2022-04-28T20:32:07.794496Z",
     "iopub.status.idle": "2022-04-28T20:32:07.804390Z",
     "shell.execute_reply": "2022-04-28T20:32:07.803226Z",
     "shell.execute_reply.started": "2022-04-28T20:32:07.794968Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22914, 1000), (5729, 1000))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking everything is working\n",
    "X_train_pad.shape, X_test_pad.shape #22914 or 5729 comments that are all 1000 tokens long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a7d60a0-d7e9-4052-8eb6-7218e7b0d4bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:14.283320Z",
     "iopub.status.busy": "2022-04-28T20:32:14.282793Z",
     "iopub.status.idle": "2022-04-28T20:32:14.293684Z",
     "shell.execute_reply": "2022-04-28T20:32:14.292241Z",
     "shell.execute_reply.started": "2022-04-28T20:32:14.283270Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#label encoder\n",
    "le = LabelEncoder()\n",
    "num_classes = 2 #toxic -> 1, non-toxic -> 0\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ce1f1f7-9ffd-4066-b509-089ce1dbc10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:15.399537Z",
     "iopub.status.busy": "2022-04-28T20:32:15.398996Z",
     "iopub.status.idle": "2022-04-28T20:32:15.406646Z",
     "shell.execute_reply": "2022-04-28T20:32:15.405687Z",
     "shell.execute_reply.started": "2022-04-28T20:32:15.399487Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create and compile model\n",
    "\n",
    "#define parameters for model\n",
    "#overall vocabulary size\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "#number of dimensions for embeddings\n",
    "EMBED_SIZE = 300\n",
    "#number of epochs to train for\n",
    "EPOCHS = 2\n",
    "#batch size for training\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ab25535-488c-41e2-824e-13c9d6d109f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:18.023518Z",
     "iopub.status.busy": "2022-04-28T20:32:18.022974Z",
     "iopub.status.idle": "2022-04-28T20:32:18.409735Z",
     "shell.execute_reply": "2022-04-28T20:32:18.409154Z",
     "shell.execute_reply.started": "2022-04-28T20:32:18.023452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 300)         7021200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1000, 128)         153728    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 500, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 500, 64)           32832     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 250, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 250, 32)           8224      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 125, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1024256   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,240,497\n",
      "Trainable params: 8,240,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 22:32:18.130184: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-04-28 22:32:18.130234: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (j-67796-job-0): /proc/driver/nvidia/version does not exist\n",
      "2022-04-28 22:32:18.285111: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "# embedding layer NEW LAYER!\n",
    "model.add(Embedding(VOCAB_SIZE, #vocabulary of certain size\n",
    "                    EMBED_SIZE, #number of dimensions for embeddings\n",
    "                    input_length=MAX_SEQUENCE_LENGTH)) #1000 characters\n",
    "\n",
    "# first convolution layer and pooling\n",
    "model.add(Conv1D(filters=128, #128 different kernels, 128 times learning\n",
    "                        kernel_size=4, \n",
    "                        padding='same',\n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2)) #max pooling= biggest value is the one being predicted\n",
    "\n",
    "# second convolution layer and pooling\n",
    "model.add(Conv1D(filters=64, #64 kernels, half of before\n",
    "                        kernel_size=4, \n",
    "                        padding='same', \n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# third convolution layer and pooling\n",
    "model.add(Conv1D(filters=32, #32 kernels, half of before\n",
    "                        kernel_size=4, \n",
    "                        padding='same', \n",
    "                        activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# fully-connected classification layer\n",
    "model.add(Flatten()) #one vector for each document\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) #only one node is wanted in output\n",
    "model.compile(loss='binary_crossentropy', #sentiments: positive/negative= binary prediction: true/false\n",
    "                        optimizer='adam', #not sgd\n",
    "                        metrics=['accuracy'])\n",
    "# print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d9782dc-f80b-423b-a4d0-e7a39af87ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:32:22.252085Z",
     "iopub.status.busy": "2022-04-28T20:32:22.251558Z",
     "iopub.status.idle": "2022-04-28T20:33:49.726920Z",
     "shell.execute_reply": "2022-04-28T20:33:49.726299Z",
     "shell.execute_reply.started": "2022-04-28T20:32:22.252034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "162/162 [==============================] - 45s 272ms/step - loss: 0.1752 - accuracy: 0.9530 - val_loss: 0.1248 - val_accuracy: 0.9629\n",
      "Epoch 2/2\n",
      "162/162 [==============================] - 43s 263ms/step - loss: 0.0695 - accuracy: 0.9750 - val_loss: 0.1093 - val_accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "history = model.fit(X_train_pad, y_train_le,\n",
    "                    epochs = EPOCHS,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    validation_split = 0.1, #usually only test/training split, but now training is getting split further\n",
    "                    verbose = True) #gives updates on screen while training\n",
    "#verbose = 0: nothing, 1: progress that updates all the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24022628-a16e-40b6-b8a5-886657dc5656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:33:49.869598Z",
     "iopub.status.busy": "2022-04-28T20:33:49.869462Z",
     "iopub.status.idle": "2022-04-28T20:33:52.524063Z",
     "shell.execute_reply": "2022-04-28T20:33:52.523581Z",
     "shell.execute_reply.started": "2022-04-28T20:33:49.869583Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 3s 14ms/step - loss: 0.0933 - accuracy: 0.9703\n",
      "Accuracy: 0.9703264236450195\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "scores = model.evaluate(X_test_pad, y_test_le, verbose=1)\n",
    "print(f\"Accuracy: {scores[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78cffbfc-5cf2-446c-804b-176ad323ebe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:33:52.525664Z",
     "iopub.status.busy": "2022-04-28T20:33:52.525426Z",
     "iopub.status.idle": "2022-04-28T20:33:52.528615Z",
     "shell.execute_reply": "2022-04-28T20:33:52.528196Z",
     "shell.execute_reply.started": "2022-04-28T20:33:52.525644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09331762790679932, 0.9703264236450195]\n"
     ]
    }
   ],
   "source": [
    "#loss value and accuracy\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51bd41d9-f11b-495d-b438-db44d3f8583f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:33:52.529589Z",
     "iopub.status.busy": "2022-04-28T20:33:52.529309Z",
     "iopub.status.idle": "2022-04-28T20:33:55.115763Z",
     "shell.execute_reply": "2022-04-28T20:33:55.115208Z",
     "shell.execute_reply.started": "2022-04-28T20:33:52.529570Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3380121e-02],\n",
       "       [3.2183528e-04],\n",
       "       [6.2187612e-03],\n",
       "       ...,\n",
       "       [4.2440891e-03],\n",
       "       [4.6268404e-03],\n",
       "       [5.7459307e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec0d49ae-d5b3-487b-b419-aecdd5be2b5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:33:55.116944Z",
     "iopub.status.busy": "2022-04-28T20:33:55.116684Z",
     "iopub.status.idle": "2022-04-28T20:33:57.778073Z",
     "shell.execute_reply": "2022-04-28T20:33:57.777583Z",
     "shell.execute_reply.started": "2022-04-28T20:33:55.116921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['non-toxic', 'non-toxic', 'non-toxic', 'toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'non-toxic', 'toxic']\n"
     ]
    }
   ],
   "source": [
    "# 0.5 decision boundary\n",
    "predictions = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "# assign labels\n",
    "predictions = [\"toxic\" if item == 1 else \"non-toxic\" for item in predictions]\n",
    "y_test = [\"toxic\" if item == 1 else \"non-toxic\" for item in y_test]\n",
    "print(predictions[:20]) #20 first predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "378789ed-4762-45e9-9846-4e679d41f909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:42:54.368404Z",
     "iopub.status.busy": "2022-04-28T20:42:54.367948Z",
     "iopub.status.idle": "2022-04-28T20:42:54.536177Z",
     "shell.execute_reply": "2022-04-28T20:42:54.535272Z",
     "shell.execute_reply.started": "2022-04-28T20:42:54.368353Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.98      0.99      0.98      5453\n",
      "       toxic       0.77      0.55      0.64       276\n",
      "\n",
      "    accuracy                           0.97      5729\n",
      "   macro avg       0.87      0.77      0.81      5729\n",
      "weighted avg       0.97      0.97      0.97      5729\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non-toxic</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>non-toxic</th>\n",
       "      <td>5408</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>125</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           non-toxic  toxic\n",
       "non-toxic       5408     45\n",
       "toxic            125    151"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix and classification report\n",
    "labels = [\"non-toxic\", \"toxic\"]\n",
    "print(classification_report(y_test, predictions))\n",
    "pd.DataFrame(confusion_matrix(y_test, predictions), \n",
    "             index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc132887-1eac-457c-8008-f84abe1aba64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-28T20:40:46.158948Z",
     "iopub.status.busy": "2022-04-28T20:40:46.158400Z",
     "iopub.status.idle": "2022-04-28T20:40:46.234321Z",
     "shell.execute_reply": "2022-04-28T20:40:46.233659Z",
     "shell.execute_reply.started": "2022-04-28T20:40:46.158898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Report has been generated and saved in the output folder as deep_learning_assign_4.txt\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions, target_names = labels)\n",
    "\n",
    "f = open(\"../../cds-lang/Lang-assignments/output/deep_learning_assign_4.txt\",'w') #saving in this folder as assign_4.1.txt\n",
    "print(report, file=f)\n",
    "\n",
    "print(\"Done! Report has been generated and saved in the output folder as deep_learning_assign_4.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
